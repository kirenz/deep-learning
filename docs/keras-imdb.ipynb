{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IMDB Sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fLCc3qLlC2u"
      },
      "source": [
        "*This tutorial is based on [An Introduction to Keras Preprocessing Layers](https://blog.tensorflow.org/2021/11/an-introduction-to-keras-preprocessing.html) by Matthew Watson and [Text classification with TensorFlow Hub: Movie reviews](https://www.tensorflow.org/tutorials/keras/text_classification_with_hub) by TensorFlow.*\n",
        " \n",
        "- Goal: build a binary sentiment classification model with keras preprocessing\n",
        "- Data: imdb movie review dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS6pA1XUlC2x"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Dataset with 50,000 polar movie reviews (positive or negative)\n",
        "- Training data and test data each 25,000 \n",
        "- Training and testing sets are balanced (contain an equal number of positive and negative reviews)\n",
        "- There is additional unlabeled data for use as well"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Split the training set into 60% and 40% \n",
        "  - 15,000 examples for training, \n",
        "  - 10,000 examples for validation\n",
        "  - 25,000 examples for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data, validation_data, test_data = tfds.load(\n",
        "    name=\"imdb_reviews\", \n",
        "    split=('train[:60%]', 'train[60%:]', 'test'),\n",
        "    as_supervised=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-04-06 21:26:24.070488: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /Users/jankirenz/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Dl Size...: 100%|██████████| 80/80 [00:21<00:00,  3.68 MiB/s]rl]\n",
            "Dl Completed...: 100%|██████████| 1/1 [00:21<00:00, 21.86s/ url]\n",
            "2022-04-06 21:27:44.856372: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDataset imdb_reviews downloaded and prepared to /Users/jankirenz/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "train_ds = tfds.load('imdb_reviews', split='train', as_supervised=True).batch(32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Keras preprocessing layers can handle a wide range of input, including structured data, images, and text. \n",
        "\n",
        "- In this case, we will be working with raw text, so we will use the TextVectorization layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "- By default, the TextVectorization layer will process text in three phases:\n",
        "\n",
        "1. First, remove punctuation and lower cases the input.\n",
        "1. Next, split text into lists of individual string words.\n",
        "1. Finally, map strings to numeric outputs using a vocabulary of known words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- A simple approach we can try here is a multi-hot encoding, where we only consider the presence or absence of terms in the review.\n",
        "- For example, say a layer vocabulary is ['movie', 'good', 'bad'], and a review read 'This movie was bad.'. We would encode this as [1, 0, 1], where movie (the first vocab term) and bad (the last vocab term) are present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_vectorizer = tf.keras.layers.TextVectorization(\n",
        "     output_mode='multi_hot', max_tokens=2500)\n",
        "\n",
        "features = train_ds.map(lambda x, y: x)\n",
        "\n",
        "text_vectorizer.adapt(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above, we create a TextVectorization layer with multi-hot output, and do two things to set the layer’s state: \n",
        "\n",
        "- First, we map over our training dataset and discard the integer label indicating a positive or negative review. \n",
        "- This gives us a dataset containing only the review text. \n",
        "- Next, we adapt() the layer over this dataset, which causes the layer to learn a vocabulary of the most frequent terms in all documents, capped at a max of 2500."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Adapt is a utility function on all stateful preprocessing layers, which allows layers to set their internal state from input data. Calling adapt is always optional. For TextVectorization, we could instead supply a precomputed vocabulary on layer construction, and skip the adapt step.\n",
        "\n",
        "We can now train a simple linear model on top of this multi-hot encoding. We will define two functions: preprocess, which converts raw input data to the representation we want for our model, and forward_pass, which applies the trainable layers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 5s 5ms/step - loss: 0.4521\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.3224\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2885A:\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2717\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 4s 5ms/step - loss: 0.2615\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f84be13da00>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def preprocess(x):\n",
        "  return text_vectorizer(x)\n",
        "\n",
        "def forward_pass(x):\n",
        "  return tf.keras.layers.Dense(1)(x)  # Linear model\n",
        "\n",
        "inputs = tf.keras.Input(shape=(1,), dtype='string')\n",
        "outputs = forward_pass(preprocess(inputs))\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True))\n",
        "model.fit(train_ds, epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That’s it for an end-to-end training example, and already enough for 85% accuracy. You can find complete code for this example at the bottom of this post.\n",
        "\n",
        "Let’s experiment with a new feature. Our multi-hot encoding does not contain any notion of review length, so we can try adding a feature for normalized string length. Preprocessing layers can be mixed with TensorFlow ops and custom layers as desired. Here we can combine the tf.strings.length function with the Normalization layer, which will scale the input to have 0 mean and 1 variance. We have only updated code up to the preprocess function below, but we will show the rest of training for clarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_apRCwI5lC2x"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J24rCPyUlC2y"
      },
      "source": [
        "**Displaying the model's summary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5TPSGfW1lC2z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization (TextVec  (None, 2500)             0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 2501      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,501\n",
            "Trainable params: 2,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This layer will scale our review length feature to mean 0 variance 1.\n",
        "normalizer = tf.keras.layers.Normalization(axis=None)\n",
        "normalizer.adapt(features.map(lambda x: tf.strings.length(x)))\n",
        "\n",
        "def preprocess(x):\n",
        "  multi_hot_terms = text_vectorizer(x)\n",
        "  normalized_length = normalizer(tf.strings.length(x))\n",
        "  # Combine the multi-hot encoding with review length.\n",
        "  return tf.keras.layers.concatenate((multi_hot_terms, normalized_length))\n",
        "\n",
        "def forward_pass(x):\n",
        "  return tf.keras.layers.Dense(1)(x)  # Linear model.\n",
        "\n",
        "inputs = tf.keras.Input(shape=(1,), dtype='string')\n",
        "outputs = forward_pass(preprocess(inputs))\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True))\n",
        "model.fit(train_ds, epochs=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above, we create the normalization layer and adapt it to our input. Within the preprocess function, we simply concatenate our multi-hot encoding and length features together. We learn a linear model over the union of the two feature representations.\n",
        "\n",
        "The last change we can make is to speed up training. We have one major opportunity to improve our training throughput. Right now, every training step, we spend some time on the CPU performing string operations (which cannot run on an accelerator), followed by calculating a loss function and gradients on a GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This gap in accelerator usage is totally unnecessary! Preprocessing is distinct from the actual forward pass of our model. The preprocessing doesn't use any of the parameters being trained. It’s a static transformation that we could precompute.\n",
        "\n",
        "To speed things up, we would like to prefetch our preprocessed batches, so that each time we are training on one batch we are preprocessing the next. This is easy to do with the tf.data library, which was built for uses like this. The only major change we need to make is to split our monolithic keras.Model into two: one for preprocessing and one for training. This is easy with Keras’ functional API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputs = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
        "preprocessed_inputs = preprocess(inputs)\n",
        "outputs = forward_pass(preprocessed_inputs)\n",
        "\n",
        "# The first model will only apply preprocessing.\n",
        "preprocessing_model = tf.keras.Model(inputs, preprocessed_inputs)\n",
        "# The second model will only apply the forward pass.\n",
        "training_model = tf.keras.Model(preprocessed_inputs, outputs)\n",
        "training_model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True))\n",
        "\n",
        "# Apply preprocessing asynchronously with tf.data.\n",
        "# It is important to call prefetch and remember the AUTOTUNE options.\n",
        "preprocessed_ds = train_ds.map(\n",
        "    lambda x, y: (preprocessing_model(x), y),\n",
        "    num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Now the GPU can focus on the training part of the model.\n",
        "training_model.fit(preprocessed_ds, epochs=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "n the above example, we pass a single keras.Input through our preprocess and forward_pass functions, but define two separate models over the transformed inputs. This slices our single graph of operations into two. Another valid option would be to only make a training model, and call the preprocess function directly when we map over our dataset. In this case, the keras.Input would need to reflect the type and shape of the preprocessed features rather than the raw strings.\n",
        "\n",
        "Using tf.data to prefetch batches cuts our train step time by over 30%! Our compute time now looks more like the following:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We could even go a step further than this, and use tf.data to cache our preprocessed dataset in memory or on disk. We would simply add a .cache() call directly before the call to prefetch. In this way, we could entirely skip computing our preprocessing batches after the first epoch of training.\n",
        "\n",
        "After training, we can rejoin our split model into a single model during inference. This allows us to save a model that can directly handle raw input data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inputs = preprocessing_model.input\n",
        "outputs = training_model(preprocessing_model(inputs))\n",
        "inference_model = tf.keras.Model(inputs, outputs)\n",
        "inference_model.predict(\n",
        "    tf.constant([\"Terrible, no good, trash.\", \"I loved this movie!\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Keras preprocessing layers aim to provide a flexible and expressive way to build data preprocessing pipelines. Prebuilt layers can be mixed and matched with custom layers and other tensorflow functions. Preprocessing can be split from training and applied efficiently with tf.data, and joined later for inference. We hope they allow for more natural and efficient iterations on feature representation in your models.\n",
        "\n",
        "To play around with the code from this post in a Colab, you can follow this link. To see a wide range of tasks you can do with preprocessing layers, see the Quick Recipes section of our preprocessing guide. You can also check out our complete tutorials for basic text classification, image data augmentation, and structured data classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFE4ieC3lC20"
      },
      "source": [
        "## Evaluation"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "chapter08_intro-to-dl-for-computer-vision.i",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
