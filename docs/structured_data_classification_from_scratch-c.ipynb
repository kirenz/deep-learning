{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iNtJdvrb_vd",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Structured data classification\n",
    "\n",
    "This tutorial is based on:\n",
    "\n",
    "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
    "**Date created:** 2020/06/09<br>\n",
    "**Last modified:** 2020/06/09<br>\n",
    "**Description:** Binary classification of structured data including numerical and categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CCfeE45b_vi",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- This example demonstrates how to do structured data classification, starting from a raw\n",
    "CSV file. \n",
    "\n",
    "- Our data includes both numerical and categorical features. \n",
    "- We will use Keras preprocessing layers to normalize the numerical features and vectorize the categorical ones.\n",
    "\n",
    "- [Our dataset](https://archive.ics.uci.edu/ml/datasets/heart+Disease) is provided by the Cleveland Clinic Foundation for Heart Disease.\n",
    "- It's a CSV file with 303 rows. \n",
    "- Each row contains information about a patient (a **sample**), and each column describes an attribute of the patient (a **feature**). \n",
    "- We use the features to predict whether a patient has a heart disease (**binary classification**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CCfeE45b_vi",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here's the description of each feature:\n",
    "\n",
    "Column| Description| Feature Type\n",
    "------------|--------------------|----------------------\n",
    "Age | Age in years | Numerical\n",
    "Sex | (1 = male; 0 = female) | Categorical\n",
    "CP | Chest pain type (0, 1, 2, 3, 4) | Categorical\n",
    "Trestbpd | Resting blood pressure (in mm Hg on admission) | Numerical\n",
    "Chol | Serum cholesterol in mg/dl | Numerical\n",
    "FBS | fasting blood sugar in 120 mg/dl (1 = true; 0 = false) | Categorical\n",
    "RestECG | Resting electrocardiogram results (0, 1, 2) | Categorical\n",
    "Thalach | Maximum heart rate achieved | Numerical\n",
    "Exang | Exercise induced angina (1 = yes; 0 = no) | Categorical\n",
    "Oldpeak | ST depression induced by exercise relative to rest | Numerical\n",
    "Slope | Slope of the peak exercise ST segment | Numerical\n",
    "CA | Number of major vessels (0-3) colored by fluoroscopy | Both numerical & categorical\n",
    "Thal | 3 = normal; 6 = fixed defect; 7 = reversible defect | Categorical\n",
    "Target | Diagnosis of heart disease (1 = true; 0 = false) | Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qAbd3kbsb_vj",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aubhVbfWb_vk",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzNNij6qb_vl",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "\n",
    "### Data import\n",
    "Let's download the data and load it into a Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EId7MS20b_vm",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n",
    "dataframe = pd.read_csv(file_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pORrkaefb_vn",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ao0iPbeFb_vn",
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The dataset includes 303 samples with 14 columns per sample (13 features, plus the target\n",
    "label):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaRl1q_ib_vo",
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here's a preview of a few samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zW3GlXdWb_vo",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzCeAJ0sb_vp",
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The last column, \"target\", indicates whether the patient has a heart disease (1) or not\n",
    "(0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data splitting\n",
    "\n",
    "Let's split the data into a training and validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OFSnqGOPb_vp",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "val_dataframe = dataframe.sample(frac=0.2, random_state=1337)\n",
    "train_dataframe = dataframe.drop(val_dataframe.index)\n",
    "\n",
    "print(\n",
    "    \"Using %d samples for training and %d for validation\"\n",
    "    % (len(train_dataframe), len(val_dataframe))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transform to Tensors\n",
    "\n",
    "The [tf.data.Dataset API](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) supports writing descriptive and efficient input pipelines. Dataset usage follows a common pattern:\n",
    "\n",
    "- Create a source dataset from your input data.\n",
    "- Apply dataset transformations to preprocess the data.\n",
    "- Iterate over the dataset and process the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# First, a simple example (use an array with values 1, 2, 3)\n",
    "example_dataset = tf.data.Dataset.from_tensor_slices([___])\n",
    "\n",
    "# Print tensor\n",
    "for element in ___:\n",
    "  print(___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Example with dictionary (call feature 1 \"a\" and feature 2 \"b\")\n",
    "example_dataset = tf.data.Dataset.from_tensor_slices({\"___\":[1, 2], \"___\":[10, 11]} )\n",
    "\n",
    "# Print tensor\n",
    "for element in ___:\n",
    "  print(___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# How to use dictionary in combination with pandas dataframe \n",
    "# We only use 1 patient (the first)\n",
    "\n",
    "example_dataset = tf.data.Dataset.from_tensor_slices(dict(dataframe[___]))\n",
    "\n",
    "for ___ in example_dataset:\n",
    "  print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2B4a8d8ab_vq",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's generate [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) objects for our training and validation dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXclBW3Cb_vq",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to create our tensors\n",
    "def dataframe_to_dataset(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop(\"___\") # use your target label\n",
    "    ds = tf.data.Dataset.from_tensor_slices((___, ___)) # first features, then label\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Use function\n",
    "# on treining data: train_dataframe\n",
    "train_ds = dataframe_to_dataset(___)\n",
    "# on validation data val_dataframe\n",
    "val_ds = dataframe_to_dataset(___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzKGzjs0b_vq",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each `Dataset` yields a tuple `(input, target)` where `input` is a dictionary of features\n",
    "and `target` is the value `0` or `1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOj1-dsZb_vr",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for x, y in train_ds.take(1):\n",
    "    print(\"Input:\", ___)\n",
    "    print(50*\"-\")    \n",
    "    print(\"Target:\", ___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2Q5DVC0b_vr",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's batch the datasets (combine some of our samples). Here, we use a mini-batch size of 32:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ccngwlq_b_vr",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = train_ds.batch(___)\n",
    "val_ds = val_ds.batch(___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWVbzdlob_vs",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature preprocessing\n",
    "\n",
    "Next, we perform feature preprocessing with Keras layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWVbzdlob_vs",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Categorical features\n",
    "\n",
    "The following features are *categorical features* encoded as integers:\n",
    "\n",
    "- `sex`\n",
    "- `cp`\n",
    "- `fbs`\n",
    "- `restecg`\n",
    "- `exang`\n",
    "- `ca`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWVbzdlob_vs",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will encode these features using **one-hot encoding**. We have two options\n",
    "here:\n",
    "\n",
    " - Use [`CategoryEncoding()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/CategoryEncoding), which requires knowing the range of input values\n",
    " and will error on input outside the range.\n",
    " - Use [`IntegerLookup()`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/IntegerLookup) which will build a lookup table for inputs and reserve\n",
    " an output index for unkown input values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWVbzdlob_vs",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For this example, we want a simple solution that will handle out of range inputs\n",
    "at inference, so we will use `IntegerLookup()`.\n",
    "\n",
    "We also have a categorical feature encoded as a *string*: `thal`. We will create an\n",
    "index of all possible features and encode output using the `StringLookup()` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWVbzdlob_vs",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Numeric features\n",
    "\n",
    "Finally, the following feature are continuous *numerical* features:\n",
    "\n",
    "- `age`\n",
    "- `trestbps`\n",
    "- `chol`\n",
    "- `thalach`\n",
    "- `oldpeak`\n",
    "- `slope`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWVbzdlob_vs",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For each of these features, we will use a [`Normalization()`](https://keras.io/api/layers/preprocessing_layers/numerical/normalization/) layer to make sure the mean\n",
    "of each feature is 0 and its standard deviation is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWVbzdlob_vs",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preprocessing functions\n",
    "\n",
    "Below, we define utility functions to do the feature preprocessing operations:\n",
    "\n",
    "- `encode_numerical_feature` to apply featurewise normalization to numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPnUDwDwb_vs",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Normalization\n",
    "\n",
    "# Define numerical preprocessing function\n",
    "def encode_numerical_feature(feature, name, dataset):\n",
    "    # Create a Normalization layer for our feature\n",
    "    normalizer = Normalization()\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the statistics of the data\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    # Normalize the input feature\n",
    "    encoded_feature = normalizer(feature)\n",
    "    return encoded_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- `encode_string_categorical_feature` to first turn string inputs into integer indices,\n",
    "then one-hot encode these integer indices.\n",
    "- `encode_integer_categorical_feature` to one-hot encode integer categorical features.\n",
    "\n",
    "We use [tf.expand_dims(input, axis, name=None)](https://www.tensorflow.org/api_docs/python/tf/expand_dims) to return a tensor with a length 1 axis inserted at index axis `-1`. A negative axis counts from the end so `axis=-1` adds an inner most dimension.\n",
    "\n",
    "During [adapt()](https://keras.io/api/layers/preprocessing_layers/categorical/string_lookup/), the layer will analyze a data set, determine the frequency of individual strings tokens, and create a vocabulary from them. If the vocabulary is capped in size, the most frequent tokens will be used to create the vocabulary and all others will be treated as out-of-vocabulary (OOV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import IntegerLookup\n",
    "from tensorflow.keras.layers import StringLookup\n",
    "\n",
    "# Define categorical preprocessing function\n",
    "def encode_categorical_feature(feature, name, dataset, is_string):\n",
    "    lookup_class = StringLookup if is_string else IntegerLookup\n",
    "    # Create a lookup layer which will turn strings into integer indices\n",
    "    lookup = lookup_class(output_mode=\"binary\")\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the set of possible string values and assign them a fixed integer index\n",
    "    lookup.adapt(feature_ds)\n",
    "\n",
    "    # Turn the string input into integer indices\n",
    "    encoded_feature = lookup(feature)\n",
    "    return encoded_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWxgxJc1b_vt",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model\n",
    "\n",
    "With this done, we can create our end-to-end model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1) Define keras.Input for every feature: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJZ0EsPjb_vt",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Categorical features encoded as integers\n",
    "sex = keras.Input(shape=(___,), name=\"___\", dtype=\"___\")\n",
    "\n",
    "cp = keras.Input(shape=(1,), name=\"cp\", dtype=\"int64\")\n",
    "fbs = keras.Input(shape=(1,), name=\"fbs\", dtype=\"int64\")\n",
    "restecg = keras.Input(shape=(1,), name=\"restecg\", dtype=\"int64\")\n",
    "exang = keras.Input(shape=(1,), name=\"exang\", dtype=\"int64\")\n",
    "ca = keras.Input(shape=(1,), name=\"ca\", dtype=\"int64\")\n",
    "\n",
    "# Categorical feature encoded as string\n",
    "thal = keras.Input(shape=(1,), name=\"thal\", dtype=\"___\")\n",
    "\n",
    "# Numerical features\n",
    "age = keras.___(shape=(1,), name=\"___\")\n",
    "\n",
    "trestbps = keras.Input(shape=(1,), name=\"trestbps\")\n",
    "chol = keras.Input(shape=(1,), name=\"chol\")\n",
    "thalach = keras.Input(shape=(1,), name=\"thalach\")\n",
    "oldpeak = keras.Input(shape=(1,), name=\"oldpeak\")\n",
    "slope = keras.Input(shape=(1,), name=\"slope\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2) Make a list of all keras.Input feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "all_inputs = [\n",
    "    sex,\n",
    "    cp,\n",
    "    fbs,\n",
    "    restecg,\n",
    "    exang,\n",
    "    ca,\n",
    "    thal,\n",
    "    age,\n",
    "    trestbps,\n",
    "    chol,\n",
    "    thalach,\n",
    "    oldpeak,\n",
    "    slope,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3) Perform preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Integer categorical features\n",
    "sex_encoded = ___(___, \"___\", train_ds, False)\n",
    "\n",
    "cp_encoded = encode_categorical_feature(cp, \"cp\", train_ds, False)\n",
    "fbs_encoded = encode_categorical_feature(fbs, \"fbs\", train_ds, False)\n",
    "restecg_encoded = encode_categorical_feature(restecg, \"restecg\", train_ds, False)\n",
    "exang_encoded = encode_categorical_feature(exang, \"exang\", train_ds, False)\n",
    "ca_encoded = encode_categorical_feature(ca, \"ca\", train_ds, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# String categorical features\n",
    "thal_encoded = encode_categorical_feature(thal, \"thal\", train_ds, ___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Numerical features\n",
    "age_encoded = ___(age, \"age\", ___)\n",
    "\n",
    "trestbps_encoded = encode_numerical_feature(trestbps, \"trestbps\", train_ds)\n",
    "chol_encoded = encode_numerical_feature(chol, \"chol\", train_ds)\n",
    "thalach_encoded = encode_numerical_feature(thalach, \"thalach\", train_ds)\n",
    "oldpeak_encoded = encode_numerical_feature(oldpeak, \"oldpeak\", train_ds)\n",
    "slope_encoded = encode_numerical_feature(slope, \"slope\", train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "all_features = layers.concatenate(\n",
    "    [\n",
    "        sex_encoded,\n",
    "        cp_encoded,\n",
    "        fbs_encoded,\n",
    "        restecg_encoded,\n",
    "        exang_encoded,\n",
    "        slope_encoded,\n",
    "        ca_encoded,\n",
    "        thal_encoded,\n",
    "        age_encoded,\n",
    "        trestbps_encoded,\n",
    "        chol_encoded,\n",
    "        thalach_encoded,\n",
    "        oldpeak_encoded,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2) Build the model \n",
    "\n",
    "1. We use 32 number of units in the first layer\n",
    "1. We use [layers.Dropout()](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) to prevent overvitting\n",
    "1. Our output layer has 1 output (since the classification task is binary)\n",
    "1. keras.Model groups layers into an object with training and inference features (provide name of all inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "x = layers.Dense(___, activation=\"relu\")(all_features)\n",
    "\n",
    "# 2\n",
    "x = ___(0.5)(___)\n",
    "\n",
    "# 3\n",
    "output = layers.Dense(___, activation=\"sigmoid\")(___)\n",
    "\n",
    "# 4\n",
    "model = keras.Model(___, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "[Model.compile](https://keras.io/api/models/model_training_apis/) configures the model for training:\n",
    "\n",
    "- [Optimizer](https://keras.io/api/optimizers/): The mechanism through which the model will update itself based on the training data it sees, so as to improve its performance. One common option for the optimizer is [Adam](https://keras.io/api/optimizers/adam/)\n",
    "\n",
    "- [loss](https://keras.io/api/losses/): The purpose of loss functions is to compute the quantity that a model should seek to minimize during training. Here, we use biary_crossentropy \n",
    "\n",
    "- [metrics](https://keras.io/api/metrics/): A metric is a function that is used to judge the performance of your model during training and testing. Here, we’ll only care about accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"___\", \n",
    "              loss =\"___\", \n",
    "              metrics=[\"___\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzkR9bceb_vu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's visualize our connectivity graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gtP52PWsb_vu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# `rankdir='LR'` is to make the graph horizontal.\n",
    "keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5igsOv1jb_vv",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Trains the model for a fixed number of epochs (iterations on a dataset).\n",
    "\n",
    "- An epoch is an arbitrary cutoff, generally defined as \"one pass over the entire dataset\", used to separate training into distinct phases, which is useful for logging and periodic evaluation. \n",
    "\n",
    "- Here, we only use 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T15LQ3qNb_vv",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(train_ds, epochs=___, validation_data=___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neb1jM3Sb_vv",
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We quickly get to 80% validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRUwMs6jb_vv",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Predictions\n",
    "\n",
    "To get a prediction for a new sample, you can simply call `model.predict()`. There are\n",
    "just two things you need to do:\n",
    "\n",
    "1. wrap scalars into a list so as to have a batch dimension (models only process batches\n",
    "of data, not single samples)\n",
    "2. Call `convert_to_tensor` on each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UV7e_9K2b_vw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sample = {\n",
    "    \"age\": 60,\n",
    "    \"sex\": 1,\n",
    "    \"cp\": 1,\n",
    "    \"trestbps\": 145,\n",
    "    \"chol\": 233,\n",
    "    \"fbs\": 1,\n",
    "    \"restecg\": 2,\n",
    "    \"thalach\": 150,\n",
    "    \"exang\": 0,\n",
    "    \"oldpeak\": 2.3,\n",
    "    \"slope\": 3,\n",
    "    \"ca\": 0,\n",
    "    \"thal\": \"fixed\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UV7e_9K2b_vw",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UV7e_9K2b_vw",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(___)\n",
    "\n",
    "print(\n",
    "    \"This particular patient had a %.1f percent probability \"\n",
    "    \"of having a heart disease, as evaluated by our model.\" % (100 * predictions[0][0],)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "structured_data_classification_from_scratch",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
