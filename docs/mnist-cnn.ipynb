{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CNN intro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fLCc3qLlC2u"
      },
      "source": [
        "*This tutorial is mainly based on the excellent book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff) by François Chollet as well as content from the [ML Practica](https://developers.google.com/machine-learning/practica) provided by Google Developers.*\n",
        "\n",
        "The problem we’re trying to solve here is to classify the grayscale images of handwritten digits (28x28 pixels) into their 10 categories (0 through 9) using a convolutional neural network (CNN).\n",
        "\n",
        "A convolutional neural network (CNN) can be used to progressively extract higher- and higher-level representations of the image content. Instead of preprocessing the data to derive features like textures and shapes, a CNN takes just the image's raw pixel data as input and \"learns\" how to extract these features, and ultimately infer what object they constitute (Google Developers)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS6pA1XUlC2x"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems.\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
        "\n",
        "Image source: [Wikipedia, Josef Steppan](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)\n",
        "\n",
        "MNIST contains a set of 60,000 training images, plus 10,000 test images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The MNIST dataset comes preloaded in Keras, in the form of a set of four NumPy\n",
        "arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our training images are stored in an array:\n",
        "\n",
        "- of shape (60000, 28, 28) \n",
        "- of type uint8 \n",
        "- with values in the [0, 255] interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# take a look at the shape\n",
        "train_images.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a look at our labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before training, we’ll preprocess the data:\n",
        "\n",
        "1. `reshape()`: we reshape our training and test data. The input is samples, height, width and depth (color channel). For a black-and-white picture, like the MNIST digits, the depth is 1 (levels of gray from value 0 to 255).\n",
        "\n",
        "1. `astype()`: we transform the data into float32\n",
        "\n",
        "1. We *scale* the data (devide by 255) so that all values are in the [0, 1] interval. \n",
        "\n",
        "Hence, we’ll transform our data into a float32 array of shape (60000, 28, 28, 1) with values between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training data\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "\n",
        "# Test data\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype(\"float32\") / 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_apRCwI5lC2x"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's first address the difference between a densely connected layer and a convolution layer in Keras:\n",
        "\n",
        "- Dense layers learn global patterns in their input feature space (for example, for a MNIST digit, patterns involving all pixels)\n",
        "\n",
        "- convolution layers learn local patterns—in the case of images, patterns found in small 2D windows of the inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature maps and filters\n",
        "\n",
        "A convolution extracts tiles of the input feature map, and applies filters to them to compute new features, producing an output feature map, or convolved feature (which may have a different size and depth than the input feature map). \n",
        "\n",
        "Convolutions are defined by two parameters ([Google Developers](https://developers.google.com/machine-learning/practica/image-classification/convolutional-neural-networks)):\n",
        "\n",
        "- Size of the tiles that are extracted (typically 3x3 or 5x5 pixels).\n",
        "- The depth of the output feature map, which corresponds to the number of filters that are applied.\n",
        "\n",
        "During a convolution, the filters (matrices the same size as the tile size) effectively slide over the input feature map's grid horizontally and vertically, one pixel at a time, extracting each corresponding tile:\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://developers.google.com/machine-learning/practica/image-classification/images/convolution_overview.gif)\n",
        "\n",
        "*Figure source: Google Developers*\n",
        "\n",
        "<br>\n",
        "\n",
        "The figure shows a 3x3 convolution of depth 1 performed over a 5x5 input feature map, also of depth 1. There are nine possible 3x3 locations to extract tiles from the 5x5 feature map, so this convolution produces a 3x3 output feature map.\n",
        "\n",
        "In summary, convolutions operate over rank-3 tensors called **feature maps**, with two spatial axes (height and width) as well as a depth axis (also called the channels axis):\n",
        "\n",
        "- feature map = (height, width, depth) *depth stands for color input*\n",
        "\n",
        "For an RGB image, the dimension of the depth axis is 3, because the image has three color channels: red, green, and blue. For a black-and-white picture, like the MNIST digits, the depth is 1 (levels of gray).\n",
        "\n",
        "The convolution operation extracts patches from its input feature map and applies the same transformation to all of these patches, producing an **output feature map**. \n",
        "\n",
        "- output feature map = (height, width, depth) *depth stands for filter* \n",
        "\n",
        "This output feature map is still a rank-3 tensor: it has a width and a height. Its depth can be arbitrary, because the output depth is a *parameter of the layer*, and the different channels in that depth axis no longer stand for specific colors as in RGB input; rather, they stand for filters. \n",
        "\n",
        "\n",
        "**Filters** encode specific aspects of the input data: at a high level, a single filter could encode the concept “presence of a face in the input,” for instance. \n",
        "\n",
        "For each filter-tile pair, the CNN performs element-wise multiplication of the filter matrix and the tile matrix, and then sums all the elements of the resulting matrix to get a single value. Each of these resulting values for every filter-tile pair is then output in the convolved feature matrix (see figures).\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://developers.google.com/machine-learning/practica/image-classification/images/convolution_example.svg)\n",
        "\n",
        "Figure source: Google Developers. Left: A 5x5 input feature map (depth 1). Right: a 3x3 convolution (depth 1).\n",
        "\n",
        "<br>\n",
        "\n",
        "![](../_static/img/feature-map.png)\n",
        "\n",
        "Figure source: Goolge Developers. Left: The 3x3 convolution is performed on the 5x5 input feature map. Right: the resulting convolved feature. Click on a value in the output feature map to see how it was calculated.\n",
        "\n",
        "<br>\n",
        "\n",
        "During training, the CNN \"learns\" the optimal values for the filter matrices that enable it to extract meaningful features (textures, edges, shapes) from the input feature map. As the number of filters (output feature map depth) applied to the input increases, so does the number of features the CNN can extract. However, the tradeoff is that filters compose the majority of resources expended by the CNN, so training time also increases as more filters are added. Additionally, each filter added to the network provides less incremental value than the previous one, so engineers aim to construct networks that use the minimum number of filters needed to extract the features necessary for accurate image classification. (Google Developers)\n",
        "\n",
        "### Pooling\n",
        "\n",
        "Pooling downsamples the convolved feature (to save on processing time), reducing the number of dimensions of the feature map, while still preserving the most critical feature information. A common algorithm used for this process is called max pooling (Google Developers):\n",
        "\n",
        "- Max pooling operates in a similar fashion to convolution. \n",
        "- We slide over the feature map and extract tiles of a specified size. \n",
        "- For each tile, the maximum value is output to a new feature map, and all other values are discarded. \n",
        "- Max pooling operations take two parameters:\n",
        "    - Size of the max-pooling filter (typically 2x2 pixels)\n",
        "    - Stride: the distance, in pixels, separating each extracted tile. Unlike with convolution, where filters slide over the feature map pixel by pixel, in max pooling, the stride determines the locations where each tile is extracted. For a 2x2 filter, a stride of 2 specifies that the max pooling operation will extract all nonoverlapping 2x2 tiles from the feature map:\n",
        "\n",
        "<br>\n",
        "\n",
        "![](https://developers.google.com/machine-learning/practica/image-classification/images/maxpool_animation.gif)\n",
        "\n",
        "*Figure source: Google Developers*\n",
        "\n",
        "<br>\n",
        "\n",
        "Figure: Left: Max pooling performed over a 4x4 feature map with a 2x2 filter and stride of 2. Right: the output of the max pooling operation. Note the resulting feature map is now 2x2, preserving only the maximum values from each tile.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Code\n",
        "\n",
        "The following code shows what a basic convnet looks like (we use the Functional API). In essence, it’s a stack of Conv2D and MaxPooling2D layers. \n",
        "\n",
        "`Input`: the convnet takes as input tensors of shape (image_height, image_width,\n",
        "image_channels), not including the batch dimension. In our case, we’ll configure the\n",
        "convnet to process inputs of size (28, 28, 1), which is the format of MNIST images. \n",
        "\n",
        "[`Conv2D`](https://keras.io/api/layers/convolution_layers/convolution2d/): the first convolution layer takes an input feature map of size (28,\n",
        "28, 1) and computes 32 filters over its input. \n",
        "\n",
        "- `kernel_size`: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. Convolution is typically done with 3x3 windows and no stride (stride 1).\n",
        "- `activation`: Following each convolution operation, the CNN applies a Rectified Linear Unit (ReLU) transformation to the convolved feature, in order to introduce nonlinearity into the model. The ReLU function, F(x) = max(0, x), returns x for all values of x > 0, and returns 0 for all values of x ≤ 0.\n",
        "\n",
        "`MaxPooling2D`: consists of extracting windows from the input feature maps and\n",
        "outputting the max value of each channel. Max pooling is usually done\n",
        "with 2x2 windows and stride 2, in order to downsample the feature maps by a factor\n",
        "of 2. \n",
        "\n",
        "After the last Conv2D layer, we end up with an output of shape (3, 3, 128): a 3x3\n",
        "feature map of 128 channels. The next step is to feed this output into a densely connected\n",
        "classifier: a stack of Dense layers. These classifiers process vectors, which are 1D, whereas the current output is a rank-3 tensor. To bridge the gap, we flatten the 3D outputs to 1D with a `Flatten` layer before adding the Dense layers.\n",
        "\n",
        "Finally, we do 10-way classification, so our last layer has 10 outputs and a softmax\n",
        "activation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5jJkEqAplC2x"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = keras.Input(shape=(28, 28, 1))\n",
        "\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J24rCPyUlC2y"
      },
      "source": [
        "**Displaying the model's summary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "5TPSGfW1lC2z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 13, 13, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 11, 11, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 5, 5, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 3, 3, 128)         73856     \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 1152)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                11530     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 104,202\n",
            "Trainable params: 104,202\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwzEs6L6lC2z"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Because we’re doing 10-way classification with a softmax output, we’ll use the categorical crossentropy loss, and because our labels are integers, we’ll use the sparse version, `sparse_categorical_crossentropy`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ap7USAEAlC20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "938/938 [==============================] - 53s 55ms/step - loss: 0.0773 - accuracy: 0.9760 - val_loss: 0.0419 - val_accuracy: 0.9858\n",
            "Epoch 2/2\n",
            "938/938 [==============================] - 58s 62ms/step - loss: 0.0390 - accuracy: 0.9876 - val_loss: 0.0262 - val_accuracy: 0.9909\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "             loss=\"sparse_categorical_crossentropy\",\n",
        "             metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(train_images, train_labels, epochs=2, \n",
        "                    validation_data = (test_images, test_labels),\n",
        "                    batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFE4ieC3lC20"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "313/313 - 2s - loss: 0.0262 - accuracy: 0.9909 - 2s/epoch - 6ms/step\n",
            "Test accuracy: 0.991\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAc/0lEQVR4nO3de5hU9Z3n8fenLwqiURS8AQaSwYCIBOmo0V1FCfMYRyGJi8AaE4lKYkZXcTZKzEXGONnsJJmMZMgFd9QwiRLFR4NuohMVl2y8rE00XvASokRajbaIrYxB+vLdP6q6KYrq7tOXU0X3+byepx7qd271PdXU73MuVecoIjAzs+yqqnQBZmZWWQ4CM7OMcxCYmWWcg8DMLOMcBGZmGecgMDPLuNSCQNL1kl6X9FQn4yVpqaQNkp6QdHRatZiZWefS3CO4ETi1i/EfB8bnHwuBH6ZYi5mZdSK1IIiItcCbXUwyG1gROQ8D+0k6JK16zMystJoKvvYoYFNBuyE/7NXiCSUtJLfXwLBhw6ZNmDChLAWamQ0W69ateyMiRpYaV8kgSCwilgPLAerq6qK+vr7CFZmZDSyS/tTZuEp+a+hlYExBe3R+mJmZlVEl9whWAxdJWgkcCzRFxC6HhczMOhUB0VbwKG4XDKezcYXzdTLNTvMmec0oMV+paboYT4maxv5nOPjIfn8bUwsCSTcD04ERkhqAq4BagIj4EfBL4DRgA/AusCCtWmwAav+QdPlB6ubD1NsPb7cfYHr24e3ydZO8ZjcdRrTl37NedHK7vGZ/dXJd1dM+b3fvUYJ6s+Zv/mlgBUFEzO9mfAB/m9br7+LtV6GpoZcf4N5+eLvqEJJ+GHrzAe7LVlEPt1DSqhdfHr00gaqKHurkeVU305caVzC85Lz5R1V15/OTcPn9UG8boo0qAtEa5NqRG9YaEFTRimjLj2uN3GOn5wGtBe3WUL4NLW258S0hWoHWEC1tyrWDjmlaQ7Tkp2tua58uN357m2htg+b8PNtbc9O1tEFLQHPrjvma26Cl41+xvS3y/+aGXdF8NPNS+F81IE4W94snfg73XlXpKrrW5Ych4Qe4qw+v1MW8Ba9bVQ2q7eLDW9Tudb1VO693T+eniw6l1+9RFaiLmnr8mknfpwR/l/bpOhERtLYFLW07/m1pbdtpWHNRu32anrZ3WkZr0NLWVrrdWrSM9nZbW8e0rW1Bc1G7ff4d7fw8rQXr0tZGJW+nUlMlqqtETZWoqa7qtF1dJWqqRU1Vfljtzu3aKjG0cHzB9O3Lq67O/TthzIh01iWVpe6OjpgNBx3Z/Ye1vz68HfMm2Spq76CsP0VER2fS0fEkaHd0mO2dTlcdXScdX2tbG81tLTt1XC1tbbt0bEnbSTvpSpGgdpeOq7hjK+jcqnPD2+fZs7YmP2/VTh1fyXZ1QYdb1K7uplNO0q6uErXVVV0us7pKaBB9ZrMTBPuPyz0yKiJyu7j5Dq+5YMtrR2eW68BaitsdW20J2rt0VAXLLGrvqKPzdrJOu/Q8FewXO7YEaws6ko520dZeriPb0dHsWVvFXgXt2qLxtR0dX9VOnWz7NMXtzpfR+RboLlukxZ1jtXbq+KuqBk+nmEXZCYIi7VuLHR1Ka8HuaXG7q93XLrb4WtvaCjrcZO2WouWX6rR3maeohs7mqaTCzmtHR1Vqi66qYNrcv3vV1PRwnu7bOzrgHR1ecQfbZbtjC3XX9mDbWrTBLzNB8K//90W+++/PdXSYrRXcXKwSvdp9bd8qG1Lbu13emuoutvAKtgK7a3e7q1/UaVd7a9Fst5aZIJh4yD7812MO67qzLNHxdXrcsovjlF2eOJJ3o81s95KZIDj+gyM4/oPpnHE3MxvIqipdgJmZVZaDwMws4xwEZmYZ5yAwM8s4B4GZWcY5CMzMMs5BYGaWcQ4CM7OMcxCYmWWcg8DMLOMcBGZmGecgMDPLOAeBmVnGOQjMzDLOQWBmlnEOAjOzjHMQmJllnIPAzCzjHARmZhnnIDAzyzgHgZlZxjkIzMwyzkFgZpZxDgIzs4xzEJiZZZyDwMws4xwEZmYZ5yAwM8u4VINA0qmSnpO0QdLiEuPfL+k+SU9IekDS6DTrMTOzXaUWBJKqgWXAx4EjgPmSjiia7DvAiog4Crga+B9p1WNmZqWluUdwDLAhIl6IiO3ASmB20TRHAPfnn68pMd7MzFKWZhCMAjYVtBvywwr9HvhU/vkngX0kHVC8IEkLJdVLqm9sbEylWDOzrKr0yeL/Dpwk6THgJOBloLV4oohYHhF1EVE3cuTIctdoZjao1aS47JeBMQXt0flhHSLiFfJ7BJL2Bs6MiLdSrMnMzIqkuUfwKDBe0jhJewDzgNWFE0gaIam9hi8D16dYj5mZlZBaEEREC3ARcA/wDHBLRDwt6WpJs/KTTQeek/Q8cBDwD2nVY2ZmpSkiKl1Dj9TV1UV9fX2lyzAzG1AkrYuIulLjKn2y2MzMKsxBYGaWcQ4CM7OMcxCYmWWcg8DMLOMcBGZmGecgMDPLOAeBmVnGOQjMzDLOQWBmlnEOAjOzjHMQmJllnIPAzCzjHARmZhnnIDAzyzgHgZlZxjkIzMwyzkFgZpZxDgIzs4xzEJiZZZyDwMws4xwEZmYZ5yAwM8s4B4GZWcY5CMzMMs5BYGaWcQ4CM7OMcxCYmWWcg8DMLOMcBGZmGecgMDPLOAeBmVnGOQjMzDLOQWBmlnGpBoGkUyU9J2mDpMUlxh8maY2kxyQ9Iem0NOsxM7NdpRYEkqqBZcDHgSOA+ZKOKJrsq8AtETEVmAf8IK16zMystDT3CI4BNkTECxGxHVgJzC6aJoD35Z/vC7ySYj1mZlZCmkEwCthU0G7IDyu0BPi0pAbgl8DFpRYkaaGkekn1jY2NadRqZpZZlT5ZPB+4MSJGA6cB/yZpl5oiYnlE1EVE3ciRI8tepJnZYNZtEEg6o1TnnMDLwJiC9uj8sELnAbcARMRDwBBgRC9ey8zMeilJBz8X+IOkf5Q0oQfLfhQYL2mcpD3InQxeXTTNS8AMAEkTyQWBj/2YmZVRt0EQEZ8GpgJ/BG6U9FD+mP0+3czXAlwE3AM8Q+7bQU9LulrSrPxkfwdcIOn3wM3AuRERfVgfMzPrISXtdyUdAJwDXEquY/8rYGlEfD+16kqoq6uL+vr6cr6kmdmAJ2ldRNSVGpfkHMEsSbcDDwC1wDER8XFgCrktejMzG8BqEkxzJvC9iFhbODAi3pV0XjplmZlZuSQJgiXAq+0NSUOBgyJiY0Tcl1ZhZmZWHkm+NXQr0FbQbs0PMzOzQSBJENTkLxEBQP75HumVZGZm5ZQkCBoLvu6JpNnAG+mVZGZm5ZTkHMEXgJ9J+hdA5K4f9JlUqzIzs7LpNggi4o/AcZL2zre3pl6VmZmVTZI9AiT9DTAJGCIJgIi4OsW6zMysTJL8oOxH5K43dDG5Q0NzgPenXJeZmZVJkpPFx0fEZ4AtEfH3wEeBw9Mty8zMyiVJEGzL//uupEOBZuCQ9EoyM7NySnKO4E5J+wHfBn5H7vaS16VZlJmZlU+XQZC/Ic19EfEWcJuku4AhEdFUjuLMzCx9XR4aiog2YFlB+z2HgJnZ4JLkHMF9ks5U+/dGzcxsUEkSBJ8nd5G59yS9LekdSW+nXJeZmZVJkl8Wd3lLSjMzG9i6DQJJJ5YaXnyjGjMzG5iSfH30SwXPhwDHAOuAU1KpyMzMyirJoaEzCtuSxgD/nFZBZmZWXklOFhdrACb2dyFmZlYZSc4RfJ/cr4khFxwfJvcLYzMzGwSSnCOoL3jeAtwcEb9NqR4zMyuzJEGwCtgWEa0Akqol7RUR76ZbmpmZlUOiXxYDQwvaQ4F70ynHzMzKLUkQDCm8PWX++V7plWRmZuWUJAj+Q9LR7Q1J04C/pFeSmZmVU5JzBJcCt0p6hdytKg8md+tKMzMbBJL8oOxRSROAD+UHPRcRzemWZWZm5ZLk5vV/CwyLiKci4ilgb0lfTL80MzMrhyTnCC7I36EMgIjYAlyQWkVmZlZWSYKguvCmNJKqgT3SK8nMzMopycniu4GfS/pxvv154FfplWRmZuWUJAiuABYCX8i3nyD3zSEzMxsEuj00lL+B/SPARnL3IjgFeCbJwiWdKuk5SRskLS4x/nuSHs8/npf0Vo+qNzOzPut0j0DS4cD8/OMN4OcAEXFykgXnzyUsA2aSu3T1o5JWR8T69mkiYlHB9BcDU3uxDmZm1gdd7RE8S27r//SI+E8R8X2gtQfLPgbYEBEvRMR2YCUwu4vp5wM392D5ZmbWD7oKgk8BrwJrJF0naQa5XxYnNQrYVNBuyA/bhaT3A+OA+zsZv1BSvaT6xsbGHpRgZmbd6TQIIuKOiJgHTADWkLvUxIGSfijpr/u5jnnAqvZLXZeoZXlE1EVE3ciRI/v5pc3Msi3JyeL/iIib8vcuHg08Ru6bRN15GRhT0B6dH1bKPHxYyMysInp0z+KI2JLfOp+RYPJHgfGSxknag1xnv7p4ovx1jIYDD/WkFjMz6x+9uXl9IhHRAlwE3EPu66a3RMTTkq6WNKtg0nnAyoiIUssxM7N0JflBWa9FxC+BXxYN+3pRe0maNZiZWddS2yMwM7OBwUFgZpZxDgIzs4xzEJiZZZyDwMws4xwEZmYZ5yAwM8s4B4GZWcY5CMzMMs5BYGaWcQ4CM7OMcxCYmWWcg8DMLOMcBGZmGecgMDPLOAeBmVnGOQjMzDLOQWBmlnEOAjOzjHMQmJllnIPAzCzjHARmZhnnIDAzyzgHgZlZxjkIzMwyzkFgZpZxDgIzs4xzEJiZZZyDwMws4xwEZmYZ5yAwM8s4B4GZWcY5CMzMMs5BYGaWcakGgaRTJT0naYOkxZ1Mc5ak9ZKelnRTmvWYmdmuatJasKRqYBkwE2gAHpW0OiLWF0wzHvgycEJEbJF0YFr1mJlZaWnuERwDbIiIFyJiO7ASmF00zQXAsojYAhARr6dYj5mZlZBmEIwCNhW0G/LDCh0OHC7pt5IelnRqqQVJWiipXlJ9Y2NjSuWamWVTpU8W1wDjgenAfOA6SfsVTxQRyyOiLiLqRo4cWd4KzcwGuTSD4GVgTEF7dH5YoQZgdUQ0R8SLwPPkgsHMzMokzSB4FBgvaZykPYB5wOqiae4gtzeApBHkDhW9kGJNZmZWJLUgiIgW4CLgHuAZ4JaIeFrS1ZJm5Se7B9gsaT2wBvhSRGxOqyYzM9uVIqLSNfRIXV1d1NfXV7oMM8trbm6moaGBbdu2VboUA4YMGcLo0aOpra3dabikdRFRV2qe1H5HYGbZ0NDQwD777MPYsWORVOlyMi0i2Lx5Mw0NDYwbNy7xfJX+1pCZDXDbtm3jgAMOcAjsBiRxwAEH9HjvzEFgZn3mENh99OZv4SAwM8s4B4GZWcY5CMzMEmppaal0Canwt4bMrN/8/Z1Ps/6Vt/t1mUcc+j6uOmNSt9N94hOfYNOmTWzbto1LLrmEhQsXcvfdd3PllVfS2trKiBEjuO+++9i6dSsXX3wx9fX1SOKqq67izDPPZO+992br1q0ArFq1irvuuosbb7yRc889lyFDhvDYY49xwgknMG/ePC655BK2bdvG0KFDueGGG/jQhz5Ea2srV1xxBXfffTdVVVVccMEFTJo0iaVLl3LHHXcA8Otf/5of/OAH3H777f36HvWVg8DMBoXrr7+e/fffn7/85S985CMfYfbs2VxwwQWsXbuWcePG8eabbwLwjW98g3333Zcnn3wSgC1btnS77IaGBh588EGqq6t5++23+c1vfkNNTQ333nsvV155JbfddhvLly9n48aNPP7449TU1PDmm28yfPhwvvjFL9LY2MjIkSO54YYb+NznPpfq+9AbDgIz6zdJttzTsnTp0o4t7U2bNrF8+XJOPPHEju/T77///gDce++9rFy5smO+4cOHd7vsOXPmUF1dDUBTUxOf/exn+cMf/oAkmpubO5b7hS98gZqamp1e75xzzuGnP/0pCxYs4KGHHmLFihX9tMb9x0FgZgPeAw88wL333stDDz3EXnvtxfTp0/nwhz/Ms88+m3gZhV+7LP4e/rBhwzqef+1rX+Pkk0/m9ttvZ+PGjUyfPr3L5S5YsIAzzjiDIUOGMGfOnI6g2J34ZLGZDXhNTU0MHz6cvfbai2effZaHH36Ybdu2sXbtWl588UWAjkNDM2fOZNmyZR3zth8aOuigg3jmmWdoa2vr8hh+U1MTo0blbq1y4403dgyfOXMmP/7xjztOKLe/3qGHHsqhhx7KNddcw4IFC/pvpfuRg8DMBrxTTz2VlpYWJk6cyOLFiznuuOMYOXIky5cv51Of+hRTpkxh7ty5AHz1q19ly5YtHHnkkUyZMoU1a9YA8K1vfYvTTz+d448/nkMOOaTT17r88sv58pe/zNSpU3f6FtH555/PYYcdxlFHHcWUKVO46aYdt2A/++yzGTNmDBMnTkzpHegbX3TOzPrkmWee2W07uN3FRRddxNSpUznvvPPK8nql/ia+6JyZWYVMmzaNYcOG8d3vfrfSpXTKQWBmlqJ169ZVuoRu+RyBmVnGOQjMzDLOQWBmlnEOAjOzjHMQmJllnIPAzDJl7733rnQJux1/fdTM+s+vFsOfn+zfZR48GT7+rf5d5m6gpaVlt7nukPcIzGxAW7x48U7XDlqyZAnXXHMNM2bM4Oijj2by5Mn84he/SLSsrVu3djrfihUrOi4fcc455wDw2muv8clPfpIpU6YwZcoUHnzwQTZu3MiRRx7ZMd93vvMdlixZAsD06dO59NJLqaur49prr+XOO+/k2GOPZerUqXzsYx/jtdde66hjwYIFTJ48maOOOorbbruN66+/nksvvbRjuddddx2LFi3q7du2s4gYUI9p06aFme0+1q9fX9HX/93vfhcnnnhiR3vixInx0ksvRVNTU0RENDY2xgc/+MFoa2uLiIhhw4Z1uqzm5uaS8z311FMxfvz4aGxsjIiIzZs3R0TEWWedFd/73vciIqKlpSXeeuutePHFF2PSpEkdy/z2t78dV111VUREnHTSSXHhhRd2jHvzzTc76rruuuvisssui4iIyy+/PC655JKdpnvnnXfiAx/4QGzfvj0iIj760Y/GE088UXI9Sv1NgPropF/dPfZLzMx6aerUqbz++uu88sorNDY2Mnz4cA4++GAWLVrE2rVrqaqq4uWXX+a1117j4IMP7nJZEcGVV165y3z3338/c+bMYcSIEcCOew3cf//9HfcXqK6uZt999+32RjftF7+D3A1v5s6dy6uvvsr27ds77p3Q2T0TTjnlFO666y4mTpxIc3MzkydP7uG7VZqDwMwGvDlz5rBq1Sr+/Oc/M3fuXH72s5/R2NjIunXrqK2tZezYsbvcY6CU3s5XqKamhra2to52V/c2uPjii7nsssuYNWsWDzzwQMchpM6cf/75fPOb32TChAn9eklrnyMwswFv7ty5rFy5klWrVjFnzhyampo48MADqa2tZc2aNfzpT39KtJzO5jvllFO49dZb2bx5M7DjXgMzZszghz/8IQCtra00NTVx0EEH8frrr7N582bee+897rrrri5fr/3eBj/5yU86hnd2z4Rjjz2WTZs2cdNNNzF//vykb0+3HARmNuBNmjSJd955h1GjRnHIIYdw9tlnU19fz+TJk1mxYgUTJkxItJzO5ps0aRJf+cpXOOmkk5gyZQqXXXYZANdeey1r1qxh8uTJTJs2jfXr11NbW8vXv/51jjnmGGbOnNnlay9ZsoQ5c+Ywbdq0jsNO0Pk9EwDOOussTjjhhES32EzK9yMwsz7x/QjK6/TTT2fRokXMmDGj02l6ej8C7xGYmQ0Ab731FocffjhDhw7tMgR6wyeLzSxznnzyyY7fArTbc889eeSRRypUUff2228/nn/++VSW7SAwsz6LCCRVuozEJk+ezOOPP17pMlLRm8P9PjRkZn0yZMgQNm/e3KsOyPpXRLB582aGDBnSo/m8R2BmfTJ69GgaGhpobGysdClGLphHjx7do3kcBGbWJ7W1tR2/iLWBKdVDQ5JOlfScpA2SFpcYf66kRkmP5x/np1mPmZntKrU9AknVwDJgJtAAPCppdUSsL5r05xFxUVp1mJlZ19LcIzgG2BARL0TEdmAlMDvF1zMzs15I8xzBKGBTQbsBOLbEdGdKOhF4HlgUEZuKJ5C0EFiYb26V9FwvaxoBvNHLeQcqr3M2eJ2zoS/r/P7ORlT6ZPGdwM0R8Z6kzwM/AU4pnigilgPL+/pikuo7+4n1YOV1zgavczaktc5pHhp6GRhT0B6dH9YhIjZHxHv55v8CpqVYj5mZlZBmEDwKjJc0TtIewDxgdeEEkg4paM4CnkmxHjMzKyG1Q0MR0SLpIuAeoBq4PiKelnQ1uVumrQb+m6RZQAvwJnBuWvXk9fnw0gDkdc4Gr3M2pLLOA+4y1GZm1r98rSEzs4xzEJiZZdygDIIEl7bYU9LP8+MfkTS2AmX2qwTrfJmk9ZKekHSfpE6/UzxQdLfOBdOdKSkkDfivGiZZZ0ln5f/WT0u6qdw19rcE/7cPk7RG0mP5/9+nVaLO/iLpekmvS3qqk/GStDT/fjwh6eg+v2hEDKoHuRPTfwQ+AOwB/B44omiaLwI/yj+fR+4yFxWvPeV1PhnYK//8wiysc366fYC1wMNAXaXrLsPfeTzwGDA83z6w0nWXYZ2XAxfmnx8BbKx03X1c5xOBo4GnOhl/GvArQMBxwCN9fc3BuEeQ5NIWs8n9eA1gFTBDA+muGrvqdp0jYk1EvJtvPkzudx0DWdJLmHwD+J/AtnIWl5Ik63wBsCwitgBExOtlrrG/JVnnAN6Xf74v8EoZ6+t3EbGW3LcoOzMbWBE5DwP7FX0Vv8cGYxCUurTFqM6miYgWoAk4oCzVpSPJOhc6j9wWxUDW7Trnd5nHRMT/LmdhKUrydz4cOFzSbyU9LOnUslWXjiTrvAT4tKQG4JfAxeUprWJ6+nnvVqUvMWFlJunTQB1wUqVrSZOkKuCfSP+3KbubGnKHh6aT2+tbK2lyRLxVyaJSNh+4MSK+K+mjwL9JOjIi2ipd2EAxGPcIur20ReE0kmrI7U5uLkt16Uiyzkj6GPAVYFbsuLTHQNXdOu8DHAk8IGkjuWOpqwf4CeMkf+cGYHVENEfEi+Qu5ji+TPWlIck6nwfcAhARDwFDyF2cbbBK9HnvicEYBN1e2iLf/mz++X8B7o/8WZgBKsnlPKYCPyYXAgP9uDF0s84R0RQRIyJibESMJXdeZFZE1Fem3H6R5P/2HeT2BpA0gtyhohfKWGN/S7LOLwEzACRNJBcEg/m+mauBz+S/PXQc0BQRr/ZlgYPu0FAku7TFv5LbfdxA7qTMvMpV3HcJ1/nbwN7Arfnz4i9FxKyKFd1HCdd5UEm4zvcAfy1pPdAKfCkiBuzebsJ1/jvgOkmLyJ04Pncgb9hJuplcmI/In/e4CqgFiIgfkTsPchqwAXgXWNDn1xzA75eZmfWDwXhoyMzMesBBYGaWcQ4CM7OMcxCYmWWcg8DMLOMcBGZFJLVKerzg0emVTXux7LGdXVXSrFIG3e8IzPrBXyLiw5UuwqxcvEdglpCkjZL+UdKTkv6fpL/KDx8r6f6Cez0clh9+kKTbJf0+/zg+v6hqSdfl7xfw75KGVmylzHAQmJUytOjQ0NyCcU0RMRn4F+Cf88O+D/wkIo4CfgYszQ9fCvyfiJhC7vryT+eHjyd3qehJwFvAmamujVk3/MtisyKStkbE3iWGbwROiYgXJNUCf46IAyS9ARwSEc354a9GxAhJjcDowgv8KXc3vF9HxPh8+wqgNiKuKcOqmZXkPQKznolOnvdE4ZVfW/G5OqswB4FZz8wt+Peh/PMH2XHhwrOB3+Sf30futqBIqpa0b7mKNOsJb4mY7WqopMcL2ndHRPtXSIdLeoLcVv38/LCLgRskfYnc5Y/brwZ5CbBc0nnktvwvBPp0uWCzNPgcgVlC+XMEdRHxRqVrMetPPjRkZpZx3iMwM8s47xGYmWWcg8DMLOMcBGZmGecgMDPLOAeBmVnG/X+V9dC6JI3J5QAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right');\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "chapter08_intro-to-dl-for-computer-vision.i",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
