{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# IMDB Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fLCc3qLlC2u",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*This tutorial is based on [An Introduction to Keras Preprocessing Layers](https://blog.tensorflow.org/2021/11/an-introduction-to-keras-preprocessing.html) by Matthew Watson, [Text classification with TensorFlow Hub: Movie reviews](https://www.tensorflow.org/tutorials/keras/text_classification_with_hub) and [Basic text classification](https://www.tensorflow.org/tutorials/keras/text_classification) by TensorFlow.*\n",
    "\n",
    "Main topics in this tutorial:\n",
    "\n",
    "- Build a binary sentiment classification model with keras \n",
    "- Use keras layers for data preprocessing\n",
    "- Use TensorBoard to view model results\n",
    "- Save and reload the model\n",
    "- Example for multiple feature engineering steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "To start this tutorial, you need the following setup:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Install [TensorFlow](https://kirenz.github.io/codelabs/codelabs/tfx-install/#0) (Note that we install TensorFlow Extended to obtain more deployment options. However, we don't use the options in this tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from keras.models import load_model\n",
    "\n",
    "from tensorboard import notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "print(\"Version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oS6pA1XUlC2x",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We use the IMDB dataset with 50,000 polar movie reviews (positive or negative)\n",
    "- Training data and test data: each 25,000 \n",
    "- Training and testing sets are balanced (they contain an equal number of positive and negative reviews)\n",
    "- The input data consists of sentences (strings)\n",
    "- The labels to predict are either 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We use 3 data splits: training, validation and test data\n",
    "- Split the data into 60% training and 40% test\n",
    "- Split training into 60% training and 40% validation\n",
    "- Resulting data split:\n",
    "  - 15,000 examples for training\n",
    "  - 10,000 examples for validation\n",
    "  - 25,000 examples for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = tfds.load(\n",
    "    name=\"imdb_reviews\", \n",
    "    split=('train[:___%]', 'train[___%:]', 'test'),\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Explore data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Each example is a sentence representing the movie review and a corresponding label. \n",
    "- The sentence is not preprocessed in any way. \n",
    "- The label is an integer value of either 0 or 1\n",
    "  - 0 is a negative review\n",
    "  - 1 is a positive review.\n",
    "- Let's print first 2 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for x, y in train_ds.take(___): \n",
    "    print(\"Input:\", x) \n",
    "    print(50*\".\")     \n",
    "    print(\"Target:\", y) \n",
    "    print(50*\"-\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- First, we need to decide how to represent the text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We will be working with raw text (natural language inputs)\n",
    "- So we will use the [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) layer.\n",
    "- It transforms a batch of strings (one example = one string) into either a\n",
    "  - list of token indices (one example = 1D tensor of integer token indices) or \n",
    "  - dense representation (one example = 1D tensor of float values representing data about the example's tokens). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) steps:\n",
    "\n",
    "1. Standardize each example (usually lowercasing + punctuation stripping)\n",
    "1. Split each example into substrings (usually words)\n",
    "1. Recombine substrings into tokens (usually ngrams)\n",
    "1. Index tokens (associate a unique int value with each token)\n",
    "1. Transform each example using this index, either into a vector of ints or a dense float vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Multi-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Multi-hot encoding: only consider the presence or absence of terms in the review.\n",
    "\n",
    "- For example: \n",
    "    - layer vocabulary is ['movie', 'good', 'bad']\n",
    "    - a review read 'This movie was bad.'\n",
    "    - We would encode this as [1, 0, 1]\n",
    "    - where movie (the first vocab term) and bad (the last vocab term) are present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. Create a `TextVectorization` layer with multi-hot output and a max of 2500 tokens\n",
    "1. Map over our training dataset and discard the integer label indicating a positive or negative review (this gives us a dataset containing only the review text) \n",
    "1. `adapt()` the layer over this dataset, which causes the layer to learn a vocabulary of the most frequent terms in all documents, capped at a max of 2500."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- `Adapt` is a utility function on all stateful preprocessing layers, which allows layers to set their internal state from input data. \n",
    "\n",
    "- Calling adapt is always optional. \n",
    "- For TextVectorization, we could instead supply a precomputed vocabulary on layer construction, and skip the adapt step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "text_vectorizer = layers.TextVectorization(\n",
    "     output_mode='____', \n",
    "     max_tokens=___\n",
    "     )\n",
    "\n",
    "features = train_ds.map(lambda x, y: x)\n",
    "\n",
    "text_vectorizer.____(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Next, we define a preprocessing function\n",
    "- This is especially useful if you combine multiple preprocessing steps\n",
    "- Here, we only use one step: `preprocess` converts raw input data to the representation we want for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(x):\n",
    "  return ____(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "inputs = keras.___(shape=(1,), dtype='string')\n",
    "\n",
    "outputs = layers.___(1)(___(___))\n",
    "\n",
    "model = keras.Model(___, ___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=['___']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Show model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.___()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let’s visualize the topology of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, \"sentiment_classifier.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, \"sentiment_classifier_with_shape_info.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can now train a simple model on top of this multi-hot encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First, we set up TensorBoard and an early stopping rule:\n",
    "\n",
    "1. Define the directory where TensorBoard stores log files (we create folders with timestamps by using [`datetime`](https://docs.python.org/3/library/datetime.html)) \n",
    "1. We add `keras.callbacks.TensorBoard` callback which ensures that logs are created and stored.\n",
    "1. To prevent overfitting, we use a callback wich will stop the training when there is no improvement in the validation accuracy for three consecutive epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create TensorBoard folders\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Create callbacks\n",
    "my_callbacks = [\n",
    "    keras.callbacks.TensorBoard(log_dir=log_dir),\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=3),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Model training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Train the model for 10 epochs in mini-batches of 512 samples\n",
    "- We shuffle the data and use a `buffer_size` of 10000\n",
    "- We monitor the model's loss and accuracy on the 10,000 samples from the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "\n",
    "*`buffer_size` is the number of items in the shuffle buffer. The function fills the buffer and then randomly samples from it. A big enough buffer is needed for proper shuffling, but it's a balance with memory consumption. Reshuffling happens automatically at every epoch*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "epochs = ___\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds.shuffle(buffer_size=___).batch(___),\n",
    "    epochs=___,\n",
    "    validation_data=val_ds.batch(___),\n",
    "    callbacks=___,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Show number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "len(history.history['loss']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loss and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Show loss and accuracy for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "results = model.evaluate(test_ds.batch(___), verbose=2)\n",
    "\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "  print(\"%s: %.3f\" % (name, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Create a plot of accuracy and loss over time\n",
    "- `model.fit()` returns a history object that contains a dictionary with everything that happened during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- There are four entries: one for each monitored metric during training and validation. \n",
    "- You can use these to plot the training and validation loss for comparison, as well as the training and validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, '___', label='Training loss')\n",
    "# r is for \"solid red line\"\n",
    "plt.plot(epochs, val_loss, '__', label='Validation loss')\n",
    "\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Blue dots represent the training loss and accuracy\n",
    "- Solid red lines are the validation loss and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Training loss decreases with each epoch \n",
    "- Training accuracy increases with each epoch. \n",
    "- This is expected when using a gradient descent optimization\n",
    "- It should minimize the desired quantity on every iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We use the tensorboard.notebook API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# View open TensorBoard instances\n",
    "notebook.list() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# If no port is provided, the most recently launched TensorBoard is used\n",
    "\n",
    "# notebook.display(port=6006, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../_static/img/tensorboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Alternative option to view TensorBoard:\n",
    "\n",
    "- How to use TensorBoard in Visual Studio Code ([Stackoverflow](https://stackoverflow.com/a/66375514)):\n",
    "\n",
    "1. Open the command palette (Ctrl/Cmd + Shift + P)\n",
    "2. Search for the command “Python: Launch TensorBoard” and press enter.\n",
    "3. Select the folder where your TensorBoard log files are located:\n",
    "   - Select folder `logs/fit` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inference on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Create new example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "examples = [\n",
    "  \"The movie was great!\",\n",
    "  \"The movie was okay.\",\n",
    "  \"The movie was terrible...\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Add a sigmoid activation layer to our model to obtain probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "probability_model = keras.Sequential([\n",
    "                        ___, \n",
    "                        layers.Activation('___')\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "probability_model.___(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A Keras model consists of multiple components:\n",
    "\n",
    "1. The architecture, or configuration, which specifies what layers the model contain, and how they're connected.\n",
    "1. A set of weights values (the \"state of the model\").\n",
    "1. An optimizer (defined by compiling the model).\n",
    "1. A set of losses and metrics (defined by compiling the model or calling add_loss() or \n",
    "add_metric())."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "The [Keras model saving API](https://keras.io/api/models/model_saving_apis/) makes it possible to save all of these pieces to disk at once, or to only selectively save some of them:\n",
    "\n",
    "- Saving everything into a single archive in the TensorFlow SavedModel format (or in the older Keras H5 format). This is the standard practice.\n",
    "- Saving the architecture / configuration only, typically as a JSON file.\n",
    "- Saving the weights values only. This is generally used when training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We will save the complete model as Tensorflow SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.___('imdb_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_new = load_model('imdb_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model_new.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multiple feature engineering steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The following code is an add on to demonstrate how to perform further feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let’s experiment with a new feature \n",
    "- Our multi-hot encoding does not contain any notion of review length\n",
    "- We can try adding a feature for normalized string length. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Create the normalization layer (which will scale the input to have 0 mean and 1 standard deviation)\n",
    "1. Adapt it to our input\n",
    "1. Within the preprocess function, we simply concatenate our multi-hot encoding and length features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# This layer will scale our review length feature to mean 0 variance 1.\n",
    "normalizer = layers.Normalization(axis=None)\n",
    "\n",
    "normalizer.adapt(features.map(lambda x: tf.strings.length(x)))\n",
    "\n",
    "def preprocess(x):\n",
    "  multi_hot_terms = text_vectorizer(x)\n",
    "  normalized_length = normalizer(tf.strings.length(x))\n",
    "  # Combine the multi-hot encoding with review length.\n",
    "  return layers.concatenate((multi_hot_terms, normalized_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Use the new preprocess function in our model (we don't use TensorBoard in this example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(1,), dtype='string')\n",
    "\n",
    "outputs = layers.Dense(1)(___(___))\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds.shuffle(buffer_size=10000).batch(512),\n",
    "    epochs=epochs,\n",
    "    validation_data=val_ds.batch(512),\n",
    "    callbacks=[callback],\n",
    "    verbose=1)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "chapter08_intro-to-dl-for-computer-vision.i",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
